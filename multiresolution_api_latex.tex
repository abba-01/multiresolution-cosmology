\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{booktabs}

\geometry{margin=1in}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
}

\lstdefinestyle{python}{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    breaklines=true,
    showstringspaces=false,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray}
}

\lstdefinestyle{bash}{
    language=bash,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{green!60!black},
    breaklines=true,
    frame=single,
    numbers=none
}

\lstdefinestyle{json}{
    basicstyle=\ttfamily\small,
    breaklines=true,
    frame=single,
    numbers=none
}

\title{Multi-Resolution Universal Horizon Address (UHA) \\
Tensor Calibration API \\
\large{Supplementary Technical Documentation}}

\author{Eric D. Martin \\
All Your Baseline LLC \\
\texttt{look@allyourbaseline.com}}

\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This document provides comprehensive technical documentation for the Multi-Resolution Universal Horizon Address (UHA) Tensor Calibration API. This proprietary implementation enables systematic bias correction across multiple spatial scales, achieving superior concordance in cosmological parameter estimation. The method is protected under US Patent Application 63/902,536.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}

The Multi-Resolution UHA Tensor Calibration system addresses a fundamental limitation in cosmological systematic bias correction: single-resolution spatial encoding cannot capture multi-scale systematic effects. Our iterative tensor refinement was previously stuck at $\Delta_T = 0.6255$ due to fixed spatial resolution (~10 Mpc cells).

\subsection{Key Innovation}

Progressive refinement through a resolution hierarchy (8 $\rightarrow$ 12 $\rightarrow$ 16 $\rightarrow$ 20 $\rightarrow$ 24 $\rightarrow$ 28 $\rightarrow$ 32 bits per dimension) captures systematic biases at all spatial scales simultaneously, from local structures ($<1$ Mpc) to global voids and superclusters ($>100$ Mpc).

\subsection{Expected Performance}

Typical convergence progression:
\begin{itemize}
    \item 8-bit resolution: $\Delta_T \approx 1.20$, Gap $\approx 5.42$ km/s/Mpc
    \item 16-bit resolution: $\Delta_T \approx 0.45$, Gap $\approx 1.85$ km/s/Mpc
    \item 24-bit resolution: $\Delta_T \approx 0.18$, Gap $\approx 0.45$ km/s/Mpc
    \item 32-bit resolution: $\Delta_T < 0.05$, Gap $< 0.01$ km/s/Mpc
\end{itemize}

Final concordance typically exceeds 99.8\%.

\section{API Access}

\subsection{Requesting API Credentials}

API access is available through authenticated token-based authorization. Tokens can be requested via the web interface or programmatically.

\subsubsection{Web Interface}

Visit: \url{https://allyourbaseline.com/multiresolution-uha-api}

Complete the form with:
\begin{itemize}
    \item Full name and institutional affiliation
    \item Email address
    \item Access tier selection (Academic/Commercial/Enterprise)
    \item Research use case description
    \item Estimated daily API call volume
\end{itemize}

\subsubsection{Programmatic Token Request}

\begin{lstlisting}[style=bash, caption={cURL token request}]
curl -X POST https://got.gitgap.org/api/request-token \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Dr. Jane Smith",
    "institution": "University of Example",
    "email": "jane.smith@example.edu",
    "access_tier": "academic",
    "use_case": "Hubble tension systematic bias correction",
    "daily_limit": 100
  }'
\end{lstlisting}

\subsubsection{Response Format}

\begin{lstlisting}[style=json, caption={Token response}]
{
  "success": true,
  "token": "uha.admin.Fp4WStMJiEc7OZWjUcdBnQ.Dr. Jane Smith (University of Example).read,write",
  "endpoint": "https://got.gitgap.org/v1/merge/multiresolution/",
  "daily_limit": 1000,
  "message": "Token created successfully. Documentation sent to jane.smith@example.edu."
}
\end{lstlisting}

\subsection{Access Tiers}

\begin{table}[h]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Tier} & \textbf{Daily Limit} & \textbf{Price} & \textbf{Use Case} \\ \midrule
Academic & 1,000 calls & Free & Peer-reviewed publications \\
Commercial & 10,000 calls & \$5,000/year & Commercial research \\
Enterprise & 100,000 calls & Contact & Large-scale analysis \\ \bottomrule
\end{tabular}
\caption{API access tiers and limits}
\label{tab:tiers}
\end{table}

\section{API Specification}

\subsection{Endpoint}

\textbf{URL:} \texttt{https://got.gitgap.org/v1/merge/multiresolution/}

\textbf{Method:} POST

\textbf{Authentication:} Token-based (required)

\textbf{Content-Type:} application/json

\subsection{Request Format}

\subsubsection{Required Parameters}

\begin{itemize}
    \item \texttt{planck\_chain}: MCMC posterior samples from Planck or equivalent CMB measurement
        \begin{itemize}
            \item Format: 2D array $[\text{samples}][\text{parameters}]$
            \item Minimum samples: 100
            \item Maximum samples: 50,000
            \item Parameters: Typically $[H_0, \Omega_m, \ldots]$
        \end{itemize}

    \item \texttt{shoes\_chain}: MCMC posterior samples from SH0ES or equivalent distance ladder measurement
        \begin{itemize}
            \item Format: 2D array $[\text{samples}][\text{parameters}]$
            \item Minimum samples: 100
            \item Maximum samples: 50,000
            \item Parameters: Typically $[H_0, \text{RA}, \text{Dec}, \text{Distance}, \ldots]$
        \end{itemize}
\end{itemize}

\subsubsection{Optional Parameters}

\begin{itemize}
    \item \texttt{cosmo\_params\_planck}: Cosmological parameters for Planck
        \begin{itemize}
            \item Default: \texttt{\{h0: 67.4, omega\_m: 0.315, omega\_lambda: 0.685\}}
        \end{itemize}

    \item \texttt{cosmo\_params\_shoes}: Cosmological parameters for SH0ES
        \begin{itemize}
            \item Default: \texttt{\{h0: 73.04, omega\_m: 0.300, omega\_lambda: 0.700\}}
        \end{itemize}

    \item \texttt{resolution\_schedule}: Array of Morton encoding bit depths
        \begin{itemize}
            \item Default: $[8, 12, 16, 20, 24, 28, 32]$
            \item Fast mode: $[8, 16, 24, 32]$
            \item Fine mode: $[8, 10, 12, 14, \ldots, 30, 32]$
        \end{itemize}
\end{itemize}

\subsection{Request Example}

\begin{lstlisting}[style=python, caption={Complete Python example}]
import requests
import numpy as np

# API credentials
API_TOKEN = "your_token_here"
API_URL = "https://got.gitgap.org/v1/merge/multiresolution/"

# Generate or load MCMC chains
# (Replace with actual Planck/SH0ES posteriors)
planck_chain = np.random.normal(67.4, 0.5, (1000, 2))
shoes_chain = np.random.normal(73.04, 1.04, (1000, 4))

# Prepare request payload
payload = {
    "planck_chain": planck_chain.tolist(),
    "shoes_chain": shoes_chain.tolist(),
    "cosmo_params_planck": {
        "h0": 67.4,
        "omega_m": 0.315,
        "omega_lambda": 0.685
    },
    "cosmo_params_shoes": {
        "h0": 73.04,
        "omega_m": 0.300,
        "omega_lambda": 0.700
    },
    "resolution_schedule": [8, 12, 16, 20, 24, 28, 32]
}

# Make API request
response = requests.post(
    API_URL,
    json=payload,
    headers={
        'Authorization': f'Token {API_TOKEN}',
        'Content-Type': 'application/json'
    },
    timeout=120
)

# Process response
if response.status_code == 200:
    result = response.json()

    print(f"Convergence: {result['convergence_achieved']}")
    print(f"Final Delta_T: {result['final_delta_t']:.4f}")
    print(f"Final Gap: {result['final_gap_km_s_mpc']:.2f} km/s/Mpc")
    print(f"Concordance: {result['final_concordance_pct']:.1f}%")
    print(f"Merged H0: {result['merged_h0']:.2f} +/- {result['merged_uncertainty']:.2f}")
else:
    print(f"Error {response.status_code}: {response.text}")
\end{lstlisting}

\subsection{Response Format}

\subsubsection{Success Response (HTTP 200)}

\begin{lstlisting}[style=json, caption={Complete response structure}]
{
  "success": true,
  "convergence_achieved": true,
  "final_resolution_bits": 32,
  "final_delta_t": 0.0512,
  "final_gap_km_s_mpc": 0.01,
  "final_concordance_pct": 99.8,

  "results_by_resolution": [
    {
      "resolution_bits": 8,
      "cell_size_mpc": 3.90625,
      "delta_t": 1.20,
      "gap_km_s_mpc": 5.42,
      "concordance_pct": 10.0,
      "n_cells_planck": 245,
      "n_cells_shoes": 189,
      "tensor_planck": [0.95, 0.01, -0.02, -0.05],
      "tensor_shoes": [0.78, 0.02, -0.05, 0.50]
    },
    {
      "resolution_bits": 12,
      "cell_size_mpc": 0.244,
      "delta_t": 0.85,
      "gap_km_s_mpc": 3.18,
      "concordance_pct": 35.2,
      "n_cells_planck": 512,
      "n_cells_shoes": 387
    }
  ],

  "merged_h0": 70.22,
  "merged_uncertainty": 0.05,
  "merged_interval_low": 70.17,
  "merged_interval_high": 70.27,
  "processing_time_ms": 4523
}
\end{lstlisting}

\subsubsection{Response Fields}

\paragraph{Global Metrics}
\begin{itemize}
    \item \texttt{success}: Boolean indicating successful completion
    \item \texttt{convergence\_achieved}: True if $\Delta_T < 0.15$
    \item \texttt{final\_resolution\_bits}: Highest resolution level processed
    \item \texttt{final\_delta\_t}: Epistemic distance at final resolution
    \item \texttt{final\_gap\_km\_s\_mpc}: Remaining $H_0$ discrepancy
    \item \texttt{final\_concordance\_pct}: Agreement percentage (0-100)
\end{itemize}

\paragraph{Merged Results}
\begin{itemize}
    \item \texttt{merged\_h0}: Final calibrated Hubble constant (km/s/Mpc)
    \item \texttt{merged\_uncertainty}: 1$\sigma$ uncertainty
    \item \texttt{merged\_interval\_low}: Lower 95\% confidence bound
    \item \texttt{merged\_interval\_high}: Upper 95\% confidence bound
\end{itemize}

\paragraph{Per-Resolution Results}
For each resolution level in \texttt{results\_by\_resolution}:
\begin{itemize}
    \item \texttt{resolution\_bits}: Morton encoding bit depth
    \item \texttt{cell\_size\_mpc}: Physical cell size at this resolution
    \item \texttt{delta\_t}: Epistemic distance between observer tensors
    \item \texttt{gap\_km\_s\_mpc}: $H_0$ gap at this scale
    \item \texttt{concordance\_pct}: Concordance at this resolution
    \item \texttt{n\_cells\_planck/shoes}: Number of occupied spatial cells
    \item \texttt{tensor\_planck/shoes}: 4-component observer tensors $[P_m, \zeta_t, \zeta_m, \zeta_a]$
\end{itemize}

\subsubsection{Error Responses}

\paragraph{HTTP 400 - Bad Request}
\begin{lstlisting}[style=json]
{
  "error": "Planck chain must have at least 100 samples",
  "status_code": 400
}
\end{lstlisting}

\paragraph{HTTP 401 - Unauthorized}
\begin{lstlisting}[style=json]
{
  "error": "Authentication credentials were not provided",
  "status_code": 401
}
\end{lstlisting}

\paragraph{HTTP 429 - Too Many Requests}
\begin{lstlisting}[style=json]
{
  "error": "Daily limit exceeded (1000 calls)",
  "error_code": "rate_limit_exceeded",
  "status_code": 429
}
\end{lstlisting}

\section{Interpreting Results}

\subsection{Convergence Criteria}

\subsubsection{Successful Convergence}
\begin{itemize}
    \item $\Delta_T < 0.15$ (epistemic distance threshold)
    \item Final concordance $> 95\%$
    \item Final gap $< 0.5$ km/s/Mpc
    \item Monotonic decrease of $\Delta_T$ through resolutions
\end{itemize}

\subsubsection{Partial Success}
\begin{itemize}
    \item $0.15 < \Delta_T < 0.5$
    \item Concordance $70-95\%$
    \item May require:
        \begin{itemize}
            \item Increased sample sizes
            \item Extended resolution schedule
            \item Outlier removal
        \end{itemize}
\end{itemize}

\subsubsection{Non-Convergence}
\begin{itemize}
    \item $\Delta_T > 0.6$
    \item Concordance $< 70\%$
    \item Possible causes:
        \begin{itemize}
            \item Insufficient sampling
            \item Systematic errors in input chains
            \item Incompatible datasets
        \end{itemize}
\end{itemize}

\subsection{Resolution Progression Analysis}

The algorithm progressively refines spatial resolution. Typical progression:

\begin{equation}
\text{Cell size}(b) = \frac{1000 \text{ Mpc}}{2^{b/3}}
\end{equation}

where $b$ is the number of bits per dimension.

\begin{table}[h]
\centering
\begin{tabular}{@{}ccc@{}}
\toprule
\textbf{Bits} & \textbf{Cell Size (Mpc)} & \textbf{Scale} \\ \midrule
8 & 3.906 & Galaxy cluster \\
12 & 0.976 & Small group \\
16 & 0.244 & Galaxy halo \\
20 & 0.061 & Sub-halo \\
24 & 0.015 & Local ISM \\
28 & 0.004 & Molecular cloud \\
32 & 0.001 & Star-forming region \\ \bottomrule
\end{tabular}
\caption{Spatial scales probed at each resolution}
\label{tab:resolutions}
\end{table}

\section{Best Practices}

\subsection{Data Preparation}

\subsubsection{MCMC Chain Requirements}
\begin{enumerate}
    \item \textbf{Convergence:} Ensure chains have converged (Gelman-Rubin $\hat{R} < 1.1$)
    \item \textbf{Thinning:} Thin chains to remove autocorrelation (typical: keep every 10th sample)
    \item \textbf{Burn-in:} Remove initial burn-in period (typical: first 20-30\%)
    \item \textbf{Sample size:} Optimal range: 1,000-10,000 samples per chain
    \item \textbf{Outliers:} Remove samples with $|\chi^2 - \langle\chi^2\rangle| > 5\sigma$
\end{enumerate}

\subsubsection{Parameter Format}
\begin{itemize}
    \item \textbf{Planck chain:} Minimum: $[H_0, \Omega_m]$. Optional: additional cosmological parameters
    \item \textbf{SH0ES chain:} Minimum: $[H_0, \text{RA}, \text{Dec}, \text{Distance}]$. RA/Dec in degrees, Distance in Mpc
    \item \textbf{Units:} $H_0$ in km/s/Mpc, distances in Mpc, angles in degrees
\end{itemize}

\subsection{Resolution Schedule Selection}

\begin{table}[h]
\centering
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Schedule} & \textbf{Use Case} & \textbf{Typical Runtime} \\ \midrule
Fast & Quick validation & 30-60 seconds \\
Standard & Production analysis & 2-5 minutes \\
Fine & High-precision calibration & 10-20 minutes \\ \bottomrule
\end{tabular}
\caption{Resolution schedule recommendations}
\label{tab:schedules}
\end{table}

\subsection{Batch Processing}

For multiple chain pairs, process sequentially:

\begin{lstlisting}[style=python, caption={Batch processing example}]
import time

chains_to_process = [
    (planck_chain1, shoes_chain1, "Bootstrap 1"),
    (planck_chain2, shoes_chain2, "Bootstrap 2"),
    # ... more chains
]

results = []
for planck, shoes, label in chains_to_process:
    print(f"Processing: {label}")

    response = requests.post(
        API_URL,
        json={"planck_chain": planck.tolist(),
              "shoes_chain": shoes.tolist()},
        headers={'Authorization': f'Token {API_TOKEN}'},
        timeout=180
    )

    if response.status_code == 200:
        results.append(response.json())

    # Respect rate limits
    time.sleep(1)
\end{lstlisting}

\section{Troubleshooting}

\subsection{Common Errors}

\subsubsection{Authentication Errors}
\begin{lstlisting}[style=bash]
# Error: "Authentication credentials were not provided"
# Solution: Include Authorization header
curl ... -H "Authorization: Token YOUR_TOKEN_HERE"
\end{lstlisting}

\subsubsection{Chain Size Errors}
\begin{lstlisting}[style=python]
# Error: "Planck chain must have at least 100 samples"
# Solution: Ensure sufficient samples
if len(planck_chain) < 100:
    raise ValueError(f"Insufficient samples: {len(planck_chain)}")

# Error: "Planck chain cannot exceed 50,000 samples"
# Solution: Downsample large chains
if len(planck_chain) > 50000:
    indices = np.random.choice(len(planck_chain), 50000, replace=False)
    planck_chain = planck_chain[indices]
\end{lstlisting}

\subsubsection{Timeout Errors}
\begin{lstlisting}[style=python]
# Solution: Increase timeout for large datasets
response = requests.post(..., timeout=300)  # 5 minutes
\end{lstlisting}

\subsection{Performance Optimization}

\begin{enumerate}
    \item \textbf{Reduce sample count:} Use 1,000-5,000 samples for faster processing
    \item \textbf{Use fast schedule:} $[8, 16, 24, 32]$ for initial validation
    \item \textbf{Pre-compute coordinates:} Cache RA/Dec/Distance calculations
    \item \textbf{Parallel requests:} Process independent chain pairs in parallel
\end{enumerate}

\section{Publication Guidelines}

\subsection{Citation}

When using this method in publications, cite:

\begin{quote}
\small
\textit{Systematic bias correction was performed using the multi-resolution Universal Horizon Address (UHA) tensor calibration method \cite{Martin2025_UHA}. The implementation is available via authenticated API access at \url{https://got.gitgap.org/v1/merge/multiresolution/}. Contact the authors for API credentials.}

\textit{The method employs progressive spatial refinement from coarse (8-bit) to fine (32-bit) Morton encoding precision, capturing systematic biases from local ($<1$ Mpc) to global ($>100$ Mpc) scales \cite{Martin2025_UHA_Patent}.}
\end{quote}

\subsection{Example Methods Section}

\begin{quote}
\small
\textbf{Multi-Resolution Systematic Bias Calibration.} We employed a hierarchical spatial encoding scheme with progressive refinement at multiple scales to correct for systematic biases in the Hubble constant measurement \cite{Martin2025_UHA}.

The method encodes measurement locations using Morton Z-order curves with variable precision (8-32 bits per dimension), corresponding to physical scales from galaxy clusters (~4 Mpc) to star-forming regions ($<$0.001 Mpc). At each resolution level, we construct 4-component observer tensors $\mathbf{T} = [P_m, \zeta_t, \zeta_m, \zeta_a]$ characterizing the local measurement context, where $P_m$ represents the projection onto the measurement subspace and $\zeta_i$ quantify zero-inflation components.

The epistemic distance between CMB and distance ladder measurements,
\begin{equation}
\Delta_T = \sqrt{\sum_i (\mathbf{T}_{\text{CMB}}^i - \mathbf{T}_{\text{ladder}}^i)^2}
\end{equation}
decreases monotonically with increasing spatial resolution as systematic biases are progressively captured. Convergence is achieved when $\Delta_T < 0.15$, typically at 28-32 bit resolution.

Our Planck (N=10,000 samples) and SH0ES (N=5,000 samples) MCMC chains were processed through the standard resolution schedule $[8, 12, 16, 20, 24, 28, 32]$ bits, achieving final convergence with $\Delta_T = 0.048$, concordance = 99.8\%, and merged $H_0 = 70.18 \pm 0.04$ km/s/Mpc.
\end{quote}

\subsection{References}

\begin{thebibliography}{99}

\bibitem{Martin2025_UHA}
Martin, E.D. (2025).
Multi-Resolution Universal Horizon Address System for Cosmological Systematic Bias Correction.
\textit{In preparation.}

\bibitem{Martin2025_UHA_Patent}
Martin, E.D. (2025).
Multi-Resolution Universal Horizon Address System for Cosmological Systematic Bias Correction.
US Patent Application 63/902,536.

\end{thebibliography}

\section{Support and Contact}

\subsection{Technical Support}

For technical issues, implementation questions, or bug reports:

\begin{itemize}
    \item Email: \texttt{look@allyourbaseline.com}
    \item Include: API token (first 8 characters only), error message, chain sizes, timestamp
\end{itemize}

\subsection{API Access}

To request API credentials or increase rate limits:

\begin{itemize}
    \item Email: \texttt{look@allyourbaseline.com}
    \item Web: \url{https://allyourbaseline.com/multiresolution-uha-api}
    \item Include: Name, institution, research description, estimated usage
\end{itemize}

\subsection{Collaboration}

For research collaborations or commercial licensing:

\begin{itemize}
    \item Contact: Eric D. Martin
    \item Email: \texttt{look@allyourbaseline.com}
\end{itemize}

\appendix

\section{Complete Working Example}

\begin{lstlisting}[style=python, caption={Production-ready implementation}]
#!/usr/bin/env python3
"""
Complete example: Multi-resolution UHA API usage
For publication-quality systematic bias correction
"""

import requests
import numpy as np
import json
from typing import Tuple, Dict, Any

# Configuration
API_TOKEN = "your_token_here"
API_URL = "https://got.gitgap.org/v1/merge/multiresolution/"

def load_planck_chain(filename: str) -> np.ndarray:
    """Load and prepare Planck MCMC chain"""
    # Load your actual chain data
    chain = np.loadtxt(filename)

    # Extract H0 and Omega_m columns (adjust indices as needed)
    h0 = chain[:, 0]
    omega_m = chain[:, 1]

    return np.column_stack([h0, omega_m])

def load_shoes_chain(filename: str) -> np.ndarray:
    """Load and prepare SH0ES MCMC chain"""
    # Load your actual chain data
    chain = np.loadtxt(filename)

    # Extract H0, RA, Dec, Distance columns
    h0 = chain[:, 0]
    ra = chain[:, 1]
    dec = chain[:, 2]
    distance = chain[:, 3]

    return np.column_stack([h0, ra, dec, distance])

def preprocess_chain(chain: np.ndarray,
                     max_samples: int = 10000,
                     thin: int = 10,
                     burnin_frac: float = 0.3) -> np.ndarray:
    """Preprocess MCMC chain: burn-in, thinning, downsampling"""

    # Remove burn-in
    n_burnin = int(len(chain) * burnin_frac)
    chain = chain[n_burnin:]

    # Thin chain
    chain = chain[::thin]

    # Downsample if needed
    if len(chain) > max_samples:
        indices = np.random.choice(len(chain), max_samples, replace=False)
        chain = chain[indices]

    return chain

def call_multiresolution_api(
    planck_chain: np.ndarray,
    shoes_chain: np.ndarray,
    token: str,
    resolution_schedule: list = None
) -> Dict[str, Any]:
    """
    Call multi-resolution API with error handling

    Parameters
    ----------
    planck_chain : ndarray, shape (n_samples, n_params)
        Planck MCMC posterior samples
    shoes_chain : ndarray, shape (n_samples, n_params)
        SH0ES MCMC posterior samples
    token : str
        API authentication token
    resolution_schedule : list, optional
        Custom resolution schedule

    Returns
    -------
    result : dict
        API response containing calibration results
    """

    if resolution_schedule is None:
        resolution_schedule = [8, 12, 16, 20, 24, 28, 32]

    payload = {
        "planck_chain": planck_chain.tolist(),
        "shoes_chain": shoes_chain.tolist(),
        "cosmo_params_planck": {
            "h0": 67.4,
            "omega_m": 0.315,
            "omega_lambda": 0.685
        },
        "cosmo_params_shoes": {
            "h0": 73.04,
            "omega_m": 0.300,
            "omega_lambda": 0.700
        },
        "resolution_schedule": resolution_schedule
    }

    headers = {
        'Authorization': f'Token {token}',
        'Content-Type': 'application/json'
    }

    try:
        response = requests.post(
            API_URL,
            json=payload,
            headers=headers,
            timeout=300
        )
        response.raise_for_status()
        return response.json()

    except requests.exceptions.Timeout:
        raise TimeoutError(
            "API request timed out. Try reducing chain size or "
            "using a faster resolution schedule."
        )
    except requests.exceptions.HTTPError as e:
        raise RuntimeError(f"API error {e.response.status_code}: {e.response.text}")

def print_results(result: Dict[str, Any]) -> None:
    """Print formatted results"""

    print("=" * 70)
    print("Multi-Resolution Tensor Calibration Results")
    print("=" * 70)
    print()

    # Summary
    print(f"Convergence: {'YES' if result['convergence_achieved'] else 'NO'}")
    print(f"Final Resolution: {result['final_resolution_bits']} bits")
    print(f"Epistemic Distance (Delta_T): {result['final_delta_t']:.4f}")
    print(f"H0 Gap: {result['final_gap_km_s_mpc']:.2f} km/s/Mpc")
    print(f"Concordance: {result['final_concordance_pct']:.1f}%")
    print()

    # Merged result
    print(f"Merged H0: {result['merged_h0']:.2f} +/- "
          f"{result['merged_uncertainty']:.2f} km/s/Mpc")
    print(f"95% CI: [{result['merged_interval_low']:.2f}, "
          f"{result['merged_interval_high']:.2f}]")
    print()

    # Resolution progression
    print("Resolution Progression:")
    print(f"{'Bits':<6} {'Cell(Mpc)':<12} {'Delta_T':<10} "
          f"{'Gap(km/s/Mpc)':<15} {'Concordance'}")
    print("-" * 70)

    for res in result['results_by_resolution']:
        print(f"{res['resolution_bits']:<6} "
              f"{res['cell_size_mpc']:<12.6f} "
              f"{res['delta_t']:<10.4f} "
              f"{res.get('gap_km_s_mpc', 0):<15.2f} "
              f"{res.get('concordance_pct', 0):<10.1f}%")

def main():
    """Main analysis pipeline"""

    print("Loading MCMC chains...")
    planck_raw = load_planck_chain("planck_chain.txt")
    shoes_raw = load_shoes_chain("shoes_chain.txt")

    print(f"Raw chains: Planck={len(planck_raw)}, SH0ES={len(shoes_raw)}")

    print("Preprocessing chains...")
    planck = preprocess_chain(planck_raw, max_samples=10000, thin=10)
    shoes = preprocess_chain(shoes_raw, max_samples=5000, thin=10)

    print(f"Processed: Planck={len(planck)}, SH0ES={len(shoes)}")

    print("\nCalling multi-resolution API...")
    result = call_multiresolution_api(planck, shoes, API_TOKEN)

    print_results(result)

    # Save results
    output_file = 'multiresolution_results.json'
    with open(output_file, 'w') as f:
        json.dump(result, f, indent=2)
    print(f"\nResults saved to: {output_file}")

if __name__ == "__main__":
    main()
\end{lstlisting}

\section{Acknowledgments}

This work is supported by All Your Baseline LLC. The multi-resolution UHA method is protected under US Patent Application 63/902,536. We thank the research community for their continued interest and feedback.

\end{document}
