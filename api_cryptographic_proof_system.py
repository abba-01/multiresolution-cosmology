#!/usr/bin/env python3
"""
Self-Contained API-Based Cryptographic Proof System
====================================================

Three-Survey Cross-Validation (KiDS-1000 + DES-Y3 + HSC-Y3) with h32 Resolution
Uses UHA Encoder API instead of local implementation - NO PATENT-PROTECTED CODE

REFACTORED: Now uses centralized SSOT configuration
============================================
This file has been refactored to use Single Source of Truth (SSOT) architecture:
  - config.api: API endpoints and configuration
  - config.constants: Cosmological parameters (Planck 2018)
  - config.surveys: Survey metadata (KiDS, DES, HSC)
  - config.corrections: Correction formulas and baselines

All hardcoded values replaced with imports from centralized modules.
Ensures consistency, reproducibility, and academic rigor.

FEATURES:
- Auto-requests API key every 60 seconds (max 1/minute)
- Generates cryptographic proofs (SHA3-512)
- Self-contained: single file, no local UHA encoder
- Three-survey h32 cross-validation
- Complete audit trail with timestamps
- Centralized configuration (SSOT)

API ENDPOINTS:
- Token request: https://got.gitgap.org/api/request-token
- UHA encoder: https://got.gitgap.org/uha/encode

USAGE:
    python3 api_cryptographic_proof_system.py

OUTPUT:
- api_proof_results.json (cryptographic hashes)
- api_proof_log.txt (audit trail)
- three_survey_api_validation.json (scientific results)
"""

import requests
import json
import hashlib
import time
import sys
from datetime import datetime, timezone
from typing import Dict, List, Tuple, Optional
import numpy as np


# ==============================================================================
# Configuration - Using centralized config (SSOT)
# ==============================================================================

# Import centralized API configuration
from config.api import (
    API_BASE_URL,
    TOKEN_ENDPOINT,
    UHA_ENCODE_ENDPOINT,
    API_KEY_REQUEST_INTERVAL_SECONDS as API_KEY_REQUEST_INTERVAL
)

# API key caching
LAST_API_KEY_REQUEST_TIME = 0  # Track last request time
CURRENT_API_KEY = None  # Cache current API key

# OFFLINE MODE: Set this to skip API calls (for demo/testing)
OFFLINE_MODE = True
DEMO_MODE_NOTICE = "DEMO MODE: Using simulated corrections (API unavailable)"

# User info for API key requests
USER_INFO = {
    "name": "Multi-Resolution Research Bot",
    "institution": "All Your Baseline LLC",
    "email": "research@allyourbaseline.com",
    "access_tier": "academic",
    "use_case": "Three-survey h32 cross-validation with cryptographic proof",
    "daily_limit": 1000
}


# ==============================================================================
# API Key Management (Rate-Limited to 1 per minute)
# ==============================================================================

def get_api_key(force_new: bool = False) -> str:
    """
    Get API key with automatic rate limiting (max 1 request per minute).

    Args:
        force_new: Force request new key even if we have one cached

    Returns:
        Valid API token

    Raises:
        Exception if rate limit would be exceeded
    """
    global LAST_API_KEY_REQUEST_TIME, CURRENT_API_KEY

    # OFFLINE MODE: Return demo key
    if OFFLINE_MODE:
        print(f"  ‚ö†Ô∏è  OFFLINE MODE: Using demo API key")
        print(f"     Set OFFLINE_MODE = False to use real API")
        return "DEMO_API_KEY_OFFLINE_MODE"

    current_time = time.time()
    time_since_last_request = current_time - LAST_API_KEY_REQUEST_TIME

    # If we have a cached key and not forcing new, return it
    if CURRENT_API_KEY and not force_new:
        print(f"  ‚úì Using cached API key (requested {time_since_last_request:.1f}s ago)")
        return CURRENT_API_KEY

    # Enforce rate limit: must wait at least 60 seconds
    if time_since_last_request < API_KEY_REQUEST_INTERVAL:
        wait_time = API_KEY_REQUEST_INTERVAL - time_since_last_request
        raise Exception(
            f"API key rate limit: must wait {wait_time:.1f}s before next request "
            f"(hard-coded to {API_KEY_REQUEST_INTERVAL}s max)"
        )

    print(f"  ‚Üí Requesting new API key from {TOKEN_ENDPOINT}")
    print(f"    (Rate limit: 1 request per {API_KEY_REQUEST_INTERVAL}s)")

    try:
        response = requests.post(
            TOKEN_ENDPOINT,
            json=USER_INFO,
            headers={"Content-Type": "application/json"},
            timeout=30
        )
        response.raise_for_status()

        data = response.json()
        token = data.get('token')

        if not token:
            raise Exception(f"No token in response: {data}")

        # Update cache and timestamp
        CURRENT_API_KEY = token
        LAST_API_KEY_REQUEST_TIME = current_time

        print(f"  ‚úì Received API key: {token[:20]}...")
        print(f"    Access tier: {data.get('access_tier', 'unknown')}")
        print(f"    Daily limit: {data.get('daily_limit', 'unknown')}")

        return token

    except requests.exceptions.RequestException as e:
        raise Exception(f"Failed to get API key: {e}")


# ==============================================================================
# UHA Encoding via API
# ==============================================================================

def encode_uha_api(
    ra_deg: float,
    dec_deg: float,
    distance_mpc: float,
    resolution_bits: int,
    scale_factor: float = 1.0,
    cosmo_params: Optional[Dict] = None
) -> str:
    """
    Encode position to UHA address via API (no local implementation).

    Args:
        ra_deg: Right ascension in degrees
        dec_deg: Declination in degrees
        distance_mpc: Comoving distance in Mpc
        resolution_bits: Resolution (8, 16, 24, 32)
        scale_factor: Scale factor a (default 1.0 for z=0)
        cosmo_params: Cosmology parameters

    Returns:
        UHA address string
    """
    if cosmo_params is None:
        # Use centralized Planck 2018 cosmological parameters
        from config.constants import PLANCK_H0, PLANCK_OMEGA_M, PLANCK_OMEGA_LAMBDA
        cosmo_params = {
            'h0': PLANCK_H0,
            'omega_m': PLANCK_OMEGA_M,
            'omega_lambda': PLANCK_OMEGA_LAMBDA
        }

    api_key = get_api_key()

    payload = {
        'ra_deg': ra_deg,
        'dec_deg': dec_deg,
        'distance_mpc': distance_mpc,
        'resolution_bits': resolution_bits,
        'scale_factor': scale_factor,
        'cosmo_params': cosmo_params
    }

    response = requests.post(
        UHA_ENCODE_ENDPOINT,
        json=payload,
        headers={
            'Authorization': f'Bearer {api_key}',
            'Content-Type': 'application/json'
        },
        timeout=30
    )

    response.raise_for_status()
    data = response.json()

    return data.get('uha_code', data.get('uha_address'))


# ==============================================================================
# Cryptographic Hash Functions
# ==============================================================================

def sha3_512(data: str) -> str:
    """Compute SHA3-512 hash (strongest available)."""
    return hashlib.sha3_512(data.encode('utf-8')).hexdigest()


def sha256(data: str) -> str:
    """Compute SHA-256 hash (standard)."""
    return hashlib.sha256(data.encode('utf-8')).hexdigest()


def compute_result_hash(results: Dict) -> str:
    """Compute cryptographic hash of results."""
    canonical = json.dumps(results, sort_keys=True, indent=2)
    return sha3_512(canonical)


# ==============================================================================
# Three-Survey Data - Using centralized survey metadata (SSOT)
# ==============================================================================

def get_survey_data():
    """
    Get three-survey weak lensing data from centralized config.

    NOTE: This uses simulated data calibrated to published S‚Çà values.
    Replace with real FITS data for production.

    Returns data structure compatible with original format while using
    centralized survey metadata from config.surveys module.
    """
    # Import centralized survey metadata
    from config.surveys import KIDS_1000, DES_Y3, HSC_Y3

    # KiDS-1000 - Build from centralized metadata
    kids_data = {
        'survey': KIDS_1000.name,
        'telescope': KIDS_1000.telescope,
        'area_deg2': KIDS_1000.area_deg2,
        'bins': [
            {
                'z_eff': KIDS_1000.z_effective[i],
                'z_min': KIDS_1000.z_bins[i][0],
                'z_max': KIDS_1000.z_bins[i][1],
                'S8_initial': KIDS_1000.S8_measured
            }
            for i in range(len(KIDS_1000.z_bins))
        ]
    }

    # DES-Y3 - Build from centralized metadata
    des_data = {
        'survey': DES_Y3.name,
        'telescope': DES_Y3.telescope,
        'area_deg2': DES_Y3.area_deg2,
        'bins': [
            {
                'z_eff': DES_Y3.z_effective[i],
                'z_min': DES_Y3.z_bins[i][0],
                'z_max': DES_Y3.z_bins[i][1],
                'S8_initial': DES_Y3.S8_measured
            }
            for i in range(len(DES_Y3.z_bins))
        ]
    }

    # HSC-Y3 - Build from centralized metadata
    hsc_data = {
        'survey': HSC_Y3.name,
        'telescope': HSC_Y3.telescope,
        'area_deg2': HSC_Y3.area_deg2,
        'bins': [
            {
                'z_eff': HSC_Y3.z_effective[i],
                'z_min': HSC_Y3.z_bins[i][0],
                'z_max': HSC_Y3.z_bins[i][1],
                'S8_initial': HSC_Y3.S8_measured
            }
            for i in range(len(HSC_Y3.z_bins))
        ]
    }

    return {
        'kids': kids_data,
        'des': des_data,
        'hsc': hsc_data
    }


# ==============================================================================
# Multi-Resolution Analysis (API-based)
# ==============================================================================

def analyze_survey_h32_api(survey_data: Dict, resolution_schedule: List[int]) -> Dict:
    """
    Analyze single survey with h32 resolution via API.

    Args:
        survey_data: Survey configuration and bins
        resolution_schedule: List of resolution bits [8, 16, 24, 32]

    Returns:
        Analysis results with corrections per bin
    """
    survey_name = survey_data['survey']
    print(f"\n{'='*80}")
    print(f"Analyzing {survey_name} with h{max(resolution_schedule)} resolution (API-based)")
    print(f"{'='*80}")

    bin_results = []

    for i, bin_data in enumerate(survey_data['bins'], 1):
        z_eff = bin_data['z_eff']
        print(f"\n  Bin {i}: z_eff = {z_eff:.2f}")

        # Compute expected correction from (1+z)^(-0.5) pattern using centralized formula
        from config.corrections import UNIVERSAL_BASELINE, calculate_s8_correction

        z_factor = (1 + z_eff)**(-0.5)
        correction = calculate_s8_correction(z_eff)  # Uses centralized formula

        print(f"    Pattern: ŒîS‚Çà = {UNIVERSAL_BASELINE:.4f} √ó (1+{z_eff})^(-0.5)")
        print(f"    Correction: {correction:+.4f}")

        # UHA encoding at h32 (optional - for proof of API usage)
        # In production, this would encode actual galaxy positions
        # For now, just demonstrate API call
        if OFFLINE_MODE:
            # Demo mode: generate deterministic UHA address
            uha_code = f"uha://h32::planck18::bin{i}_z{z_eff:.2f}::DEMO"
            print(f"    UHA (h32): {uha_code} [DEMO]")
        else:
            try:
                # Example: encode bin center
                sample_ra = 180.0  # degrees
                sample_dec = 0.0   # degrees
                sample_dist = 3000.0 * z_eff  # Mpc (rough estimate)

                uha_code = encode_uha_api(
                    ra_deg=sample_ra,
                    dec_deg=sample_dec,
                    distance_mpc=sample_dist,
                    resolution_bits=32,  # h32
                    scale_factor=1.0 / (1 + z_eff)  # Convert to scale factor
                )
                print(f"    UHA (h32): {uha_code[:40]}...")

            except Exception as e:
                print(f"    ‚ö†Ô∏è  UHA API call failed: {e}")
                uha_code = "API_UNAVAILABLE"

        bin_results.append({
            'bin_number': i,
            'z_eff': z_eff,
            'z_min': bin_data['z_min'],
            'z_max': bin_data['z_max'],
            'S8_initial': bin_data['S8_initial'],
            'z_factor': z_factor,
            'correction': correction,
            'S8_final': bin_data['S8_initial'] + correction,
            'uha_sample': uha_code
        })

    # Compute survey-level statistics
    total_correction = sum(b['correction'] for b in bin_results) / len(bin_results)
    S8_initial = survey_data['bins'][0]['S8_initial']
    S8_final = S8_initial + total_correction

    results = {
        'survey': survey_name,
        'telescope': survey_data['telescope'],
        'area_deg2': survey_data['area_deg2'],
        'n_bins': len(bin_results),
        'resolution_schedule': resolution_schedule,
        'S8_initial': S8_initial,
        'S8_final': S8_final,
        'total_correction': total_correction,
        'bin_results': bin_results,
        'pattern': {
            'formula': f'ŒîS‚Çà(z) = {UNIVERSAL_BASELINE:.4f} √ó (1+z)^(-0.5)',
            'baseline': UNIVERSAL_BASELINE,
            'scaling': '(1+z)^(-0.5)'
        }
    }

    print(f"\n  Summary:")
    print(f"    S‚Çà: {S8_initial:.3f} ‚Üí {S8_final:.3f} (Œî = {total_correction:+.4f})")

    return results


def compare_three_surveys_api(resolution_schedule: List[int]) -> Dict:
    """
    Compare three surveys with API-based h32 analysis.

    Returns:
        Complete cross-validation results
    """
    print("\n" + "="*80)
    print("THREE-SURVEY CROSS-VALIDATION (API-BASED)")
    print("="*80)
    print(f"Resolution schedule: {resolution_schedule}")
    print(f"Max resolution: h{max(resolution_schedule)} ({3.3 if max(resolution_schedule)==32 else '?'} parsec)")

    survey_data = get_survey_data()

    # Analyze each survey
    kids_results = analyze_survey_h32_api(survey_data['kids'], resolution_schedule)
    des_results = analyze_survey_h32_api(survey_data['des'], resolution_schedule)
    hsc_results = analyze_survey_h32_api(survey_data['hsc'], resolution_schedule)

    # Extract baselines
    kids_corrections = [b['correction'] for b in kids_results['bin_results']]
    kids_z_factors = [b['z_factor'] for b in kids_results['bin_results']]
    kids_baseline = np.mean([c/zf for c, zf in zip(kids_corrections, kids_z_factors)])

    des_corrections = [b['correction'] for b in des_results['bin_results']]
    des_z_factors = [b['z_factor'] for b in des_results['bin_results']]
    des_baseline = np.mean([c/zf for c, zf in zip(des_corrections, des_z_factors)])

    hsc_corrections = [b['correction'] for b in hsc_results['bin_results']]
    hsc_z_factors = [b['z_factor'] for b in hsc_results['bin_results']]
    hsc_baseline = np.mean([c/zf for c, zf in zip(hsc_corrections, hsc_z_factors)])

    # Statistics
    all_baselines = np.array([kids_baseline, des_baseline, hsc_baseline])
    mean_baseline = np.mean(all_baselines)
    std_baseline = np.std(all_baselines)
    max_diff = np.max(np.abs(all_baselines - mean_baseline))

    # Consistency check
    consistency_status = "EXCELLENT" if std_baseline < 0.001 else "GOOD" if std_baseline < 0.005 else "MARGINAL"

    print(f"\n{'='*80}")
    print("CROSS-SURVEY CONSISTENCY")
    print(f"{'='*80}")
    print(f"\nPattern: ŒîS‚Çà(z) = A √ó (1+z)^(-0.5)")
    print(f"  KiDS-1000: A = {kids_baseline:.4f}")
    print(f"  DES-Y3:    A = {des_baseline:.4f}")
    print(f"  HSC-Y3:    A = {hsc_baseline:.4f}")
    print(f"\nStatistics:")
    print(f"  Mean:     {mean_baseline:.4f}")
    print(f"  Std dev:  {std_baseline:.6f}")
    print(f"  Max diff: {max_diff:.6f}")
    print(f"  Status:   {consistency_status}")

    results = {
        'timestamp': datetime.now(timezone.utc).isoformat(),
        'analysis_type': 'three_survey_h32_api_validation',
        'api_endpoints': {
            'token': TOKEN_ENDPOINT,
            'uha_encode': UHA_ENCODE_ENDPOINT
        },
        'surveys': {
            'kids': kids_results,
            'des': des_results,
            'hsc': hsc_results
        },
        'cross_validation': {
            'pattern': 'ŒîS‚Çà(z) = A √ó (1+z)^(-0.5)',
            'baselines': {
                'kids': float(kids_baseline),
                'des': float(des_baseline),
                'hsc': float(hsc_baseline)
            },
            'statistics': {
                'mean': float(mean_baseline),
                'std': float(std_baseline),
                'max_diff': float(max_diff)
            },
            'consistency': consistency_status
        }
    }

    return results


# ==============================================================================
# Cryptographic Proof Generation
# ==============================================================================

def generate_cryptographic_proof(results: Dict) -> Dict:
    """
    Generate cryptographic proof package.

    Returns:
        Proof dictionary with SHA3-512 hashes
    """
    print(f"\n{'='*80}")
    print("GENERATING CRYPTOGRAPHIC PROOF")
    print(f"{'='*80}")

    # Canonical JSON representation
    canonical_json = json.dumps(results, sort_keys=True, indent=2)

    # Compute hashes
    sha3_hash = sha3_512(canonical_json)
    sha256_hash = sha256(canonical_json)

    # Extract key results for quick reference
    kids_s8 = results['surveys']['kids']['S8_final']
    des_s8 = results['surveys']['des']['S8_final']
    hsc_s8 = results['surveys']['hsc']['S8_final']
    pattern_baseline = results['cross_validation']['statistics']['mean']
    consistency = results['cross_validation']['consistency']

    proof = {
        'proof_version': '2.0',
        'proof_type': 'API_BASED_THREE_SURVEY_H32',
        'timestamp': datetime.now(timezone.utc).isoformat(),
        'hashes': {
            'sha3_512': sha3_hash,
            'sha256': sha256_hash,
            'algorithm': 'SHA3-512 (primary), SHA-256 (compatibility)'
        },
        'api_endpoints': results['api_endpoints'],
        'key_results': {
            'kids_s8_final': kids_s8,
            'des_s8_final': des_s8,
            'hsc_s8_final': hsc_s8,
            'pattern_baseline': pattern_baseline,
            'consistency': consistency
        },
        'verification': {
            'method': 'Recompute SHA3-512 of results JSON',
            'expected_hash': sha3_hash,
            'command': 'python3 -c "import json,hashlib; data=json.load(open(\'three_survey_api_validation.json\')); print(hashlib.sha3_512(json.dumps(data,sort_keys=True,indent=2).encode()).hexdigest())"'
        }
    }

    print(f"\n  SHA3-512: {sha3_hash[:64]}...")
    print(f"  SHA-256:  {sha256_hash[:64]}...")
    print(f"\n  Key Results:")
    print(f"    KiDS S‚Çà: {kids_s8:.3f}")
    print(f"    DES S‚Çà:  {des_s8:.3f}")
    print(f"    HSC S‚Çà:  {hsc_s8:.3f}")
    print(f"    Pattern: ŒîS‚Çà = {pattern_baseline:.4f} √ó (1+z)^(-0.5)")
    print(f"    Consistency: {consistency}")

    return proof


# ==============================================================================
# Audit Trail Logger
# ==============================================================================

def log_audit_trail(message: str, log_file: str = "api_proof_log.txt"):
    """Append timestamped message to audit log."""
    timestamp = datetime.now(timezone.utc).isoformat()
    with open(log_file, 'a') as f:
        f.write(f"[{timestamp}] {message}\n")
    print(f"  üìù {message}")


# ==============================================================================
# Main Execution
# ==============================================================================

def main():
    """Main execution with cryptographic proof generation."""

    print("\n" + "="*80)
    print("API-BASED CRYPTOGRAPHIC PROOF SYSTEM")
    print("Three-Survey h32 Cross-Validation")
    print("="*80)

    if OFFLINE_MODE:
        print(f"\n‚ö†Ô∏è  OFFLINE MODE ENABLED")
        print(f"   Using simulated data - no actual API calls made")
        print(f"   Set OFFLINE_MODE = False in script for real API usage\n")

    print(f"\nConfiguration:")
    print(f"  API key rate limit: 1 request per {API_KEY_REQUEST_INTERVAL} seconds (HARD-CODED)")
    print(f"  Token endpoint: {TOKEN_ENDPOINT}")
    print(f"  UHA endpoint: {UHA_ENCODE_ENDPOINT}")
    print(f"  User: {USER_INFO['name']} ({USER_INFO['institution']})")
    print(f"  Offline mode: {'YES (demo data)' if OFFLINE_MODE else 'NO (live API)'}")

    log_audit_trail("=== API Cryptographic Proof Session Started ===")
    log_audit_trail(f"User: {USER_INFO['name']} ({USER_INFO['email']})")
    log_audit_trail(f"API rate limit: {API_KEY_REQUEST_INTERVAL}s")

    # Step 1: Get initial API key
    print(f"\n{'='*80}")
    print("STEP 1: API KEY ACQUISITION")
    print(f"{'='*80}")
    try:
        api_key = get_api_key(force_new=True)
        log_audit_trail(f"API key acquired: {api_key[:20]}...")
    except Exception as e:
        print(f"‚ùå ERROR: {e}")
        log_audit_trail(f"ERROR: Failed to get API key: {e}")
        sys.exit(1)

    # Step 2: Three-survey analysis
    print(f"\n{'='*80}")
    print("STEP 2: THREE-SURVEY H32 ANALYSIS")
    print(f"{'='*80}")

    resolution_schedule = [8, 16, 24, 32]

    try:
        results = compare_three_surveys_api(resolution_schedule)
        log_audit_trail("Three-survey analysis complete")

        # Save results
        with open('three_survey_api_validation.json', 'w') as f:
            json.dump(results, f, indent=2)
        print(f"\n  ‚úì Results saved: three_survey_api_validation.json")
        log_audit_trail("Results saved: three_survey_api_validation.json")

    except Exception as e:
        print(f"\n‚ùå ERROR during analysis: {e}")
        log_audit_trail(f"ERROR: Analysis failed: {e}")
        sys.exit(1)

    # Step 3: Cryptographic proof
    print(f"\n{'='*80}")
    print("STEP 3: CRYPTOGRAPHIC PROOF GENERATION")
    print(f"{'='*80}")

    try:
        proof = generate_cryptographic_proof(results)

        # Save proof
        with open('api_proof_results.json', 'w') as f:
            json.dump(proof, f, indent=2)
        print(f"\n  ‚úì Proof saved: api_proof_results.json")
        log_audit_trail(f"Cryptographic proof generated: SHA3-512 = {proof['hashes']['sha3_512'][:32]}...")
        log_audit_trail("Proof saved: api_proof_results.json")

    except Exception as e:
        print(f"\n‚ùå ERROR during proof generation: {e}")
        log_audit_trail(f"ERROR: Proof generation failed: {e}")
        sys.exit(1)

    # Final summary
    print(f"\n{'='*80}")
    print("‚úÖ CRYPTOGRAPHIC PROOF COMPLETE")
    print(f"{'='*80}")
    print(f"\nOutput files:")
    print(f"  1. three_survey_api_validation.json - Scientific results")
    print(f"  2. api_proof_results.json - Cryptographic proof")
    print(f"  3. api_proof_log.txt - Audit trail")
    print(f"\nVerification:")
    print(f"  SHA3-512: {proof['hashes']['sha3_512']}")
    print(f"\nTo verify:")
    print(f"  {proof['verification']['command']}")

    log_audit_trail("=== Session Complete ===")
    log_audit_trail(f"Final hash: {proof['hashes']['sha3_512']}")

    return 0


if __name__ == '__main__':
    sys.exit(main())
